{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval_similaritymatrix_summarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIm3yP9OH76S5onAekAieS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-miramontes/News_Filter/blob/master/eval_similaritymatrix_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjxRzXNmwvY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcoixOdo--Q4",
        "colab_type": "text"
      },
      "source": [
        "# Load Universal Sentence Encoder and Training Data for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQKnGZsK_OzT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c5867715-b74d-413e-baf5-af279a8a5d80"
      },
      "source": [
        "# import the training data\n",
        "small_data = pd.read_csv(\"\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-154d1d1b165b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_cs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_cs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsu8hNC6_P4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download model from https://tfhub.dev/google/universal-sentence-encoder/4 and save locally \n",
        "emb_model = hub.model(\"tmp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zPMrq1q_Ywc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduce logging output\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "# compute embeddings for each article\n",
        "train_embeddings = emb_model(small_data.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MXmAstxC06r",
        "colab_type": "text"
      },
      "source": [
        "# Create Summaries for Clusters from Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z21OrgsbID5a",
        "colab_type": "text"
      },
      "source": [
        "Some functions to make this possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWMNzmaI3gXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters = pd.read_csv(\"clusters.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjmLxmyfITy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_content(piece):\n",
        "  article = piece.split(\". \")\n",
        "  sentences = []\n",
        "\n",
        "  for sentence in article:\n",
        "    #print(sentence)\n",
        "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    #sentences.pop()\n",
        "\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTXzmguDIT72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# measuring similarity\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "  if stopwords is None:\n",
        "    stopwords=[]\n",
        "\n",
        "  sent1 = [w.lower() for w in sent1]\n",
        "  sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "  all_words = list(set(sent1 + sent2))\n",
        "\n",
        "  vector1 = [0] * len(all_words)\n",
        "  vector2 = [0] * len(all_words)\n",
        "\n",
        "  # build vector for the first sentence\n",
        "  for w in sent1:\n",
        "    if w in stopwords:\n",
        "      continue\n",
        "    vector1[all_words.index(w)] += 1\n",
        "\n",
        "  # build vector for second sentence\n",
        "  for w in sent2:\n",
        "    if w in stopwords:\n",
        "      continue\n",
        "    vector2[all_words.index(w)] += 1\n",
        "\n",
        "  return 1 - cosine_distance(vector1, vector2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcHUdJCBIUDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# similarity matrix: cos sim to find similarity btw sent\n",
        "def build_sim_matrix(sentences, stop_words):\n",
        "  # create empty sim matrix\n",
        "  sim_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "      if i == j: # skip if both are same sent\n",
        "        continue\n",
        "      sim_matrix[i][j] = sentence_similarity(sentences[i],\n",
        "                                             sentences[j],\n",
        "                                             stop_words)\n",
        "  return sim_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxr41QnBIgCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate summary method\n",
        "def generate_summary(content, top_n=5):\n",
        "  stop_words = stopwords.words('english')\n",
        "  summarized_text=[]\n",
        "\n",
        "  # first read in article and tokenize\n",
        "  sentences = read_content(content)\n",
        "\n",
        "  # second generate sim matrix accross sents\n",
        "  sent_sim_mat = build_sim_matrix(sentences, stop_words)\n",
        "\n",
        "  # third rank sentences in sim matrix\n",
        "  sent_sim_graph = nx.from_numpy_array(sent_sim_mat)\n",
        "\n",
        "  scores = nx.pagerank(sent_sim_graph, max_iter=5000)\n",
        "\n",
        "  # fourth sort the rank and pick top sent\n",
        "  ranked_sent = sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
        "                       reverse=True)\n",
        "\n",
        "  for i in range(top_n):\n",
        "    summarized_text.append(\" \".join(ranked_sent[i][1]))\n",
        "\n",
        "  # fifth, output the sumarized text\n",
        "  output = \". \".join(summarized_text)\n",
        "  #print(\"Summarized Text: \\n\", \". \".join(summarized_text))\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je64LLhwIgK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_summaries(cluster_name):\n",
        "  '''\n",
        "  input: list of articles in cluster\n",
        "  output: dictionary with valid summaries\n",
        "  '''\n",
        "\n",
        "  summa = {}\n",
        "  for j in range(len(cluster_name)):\n",
        "    try:\n",
        "      out = generate_summary(cluster_name[j])\n",
        "      summa[j] = out\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "  return summa\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1DFbUgzk9YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summary_summaries(cluster):\n",
        "  \"\"\"\n",
        "  Recall that each cluster is a dictionary\n",
        "  Key: Article number\n",
        "  Values: Text\n",
        "\n",
        "  Thus, cluster - dictionary\n",
        "  \"\"\"\n",
        "  articles = cluster.values()\n",
        "  concat_ = \"\"\n",
        "  for article in articles:\n",
        "    concat_ += article\n",
        "  \n",
        "  summary = generate_summary(concat_)\n",
        "\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uwqE7JVlrkV",
        "colab_type": "text"
      },
      "source": [
        "# Generation of Summaries per Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3hKdpymlx_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarizing based on the given clusters\n",
        "\n",
        "cluster_summaries = []\n",
        "for i in range(1,6):\n",
        "  # argument below is the list of articles for each cluster\n",
        "  summaries = get_summaries(clusters[clusters.cluster_labels == i].reset_index().content)\n",
        "  cluster_summaries.append(summaries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy33pbur363O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_summaries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pj7Qc5Kvo1j",
        "colab_type": "text"
      },
      "source": [
        "# Create Summary of Summaries for each Cluster\n",
        "Since there are five clusters, each of them with summaries of relevant artciles, we now concatenate and return a final summary per cluster. The following operation yields a single summary per cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HShrAq2EzSDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_of_summaries = []\n",
        "for summaries in cluster_summaries:\n",
        "  # this is a summary of concatenated text\n",
        "  summario = summary_summaries(summaries)\n",
        "  summary_of_summaries.append(summario)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCNuMY8gzZ1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making sure that the number of clusters, matches the summaries\n",
        "len(cluster_summaries) == len(summary_of_summaries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_wC5GyR4G_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_of_summaries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xj8YN1rzgPa",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation of Summaries\n",
        "Goal: Each summary is clustered with the original articles that were used to create the summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rQzCLh_0OUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create embeddings for each summary\n",
        "summary_embeddings = emb_model(summary_of_summaries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4SpdvbQ0akn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create DF with titles on axis, and semantic similarities in cells\n",
        "cos_df = pd.DataFrame(cosine_similarity(summary_embeddings, small_embeddings))\n",
        "cos_df.columns = small_data.title\n",
        "cos_df.index = [summary_of_summaries[i][:50] for i in range(len(summary_of_summaries))]\n",
        "\n",
        "cos_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl27iryC0iw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func to return the column index of the top n values in a row of a dataframe\n",
        "\n",
        "def find_topind(df, i ,n):\n",
        "  return list(list(zip(*heapq.nlargest(n, enumerate(df.iloc[i,:]), key=operator.itemgetter(1))))[0])\n",
        "\n",
        "# function to return the top n values in a list\n",
        "def find_top(lst, ind):\n",
        "  return [lst[i] for i in ind]\n",
        "\n",
        "# how many articles per cluster\n",
        "n = 10\n",
        "\n",
        "#find index of n most similar articles\n",
        "top_ind = Parallel(n_jobs=16)(delayed(find_topind)(cos_df, i, n) for i in range(len(cos_df)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoSn_juJ0pKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ids of most similar articles \n",
        "top_id = Parallel(n_jobs=16)(delayed(find_top)(small_data.id, ind) for ind in top_ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ZLUuZR0tZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ids of original articles  \n",
        "og_ids = []\n",
        "for i in range(1,6):\n",
        "  cluster = clusters[clusters.cluster_labels == i]\n",
        "  og_ids.append(list(cluster.id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BDDhGY90y1M",
        "colab_type": "text"
      },
      "source": [
        "So what is the percentage in accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNH3b50A02tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# proportion of original articles clustered with summaries \n",
        "np.mean([sum([id in top_id[i] for id in og_ids[i]])/10 for i in range(len(og_ids))])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}